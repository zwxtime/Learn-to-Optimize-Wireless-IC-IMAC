#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–çš„Pythonä»£ç ï¼Œç”¨äºå¤§è§„æ¨¡ç”ŸæˆIMACæ•°æ®é›†
é’ˆå¯¹100ä¸‡æ ·æœ¬ç”Ÿæˆè¿›è¡Œäº†æ€§èƒ½ä¼˜åŒ–
åŒ…å«æ‰¹é‡å­˜å‚¨å’Œæ€§èƒ½ç›‘æ§åŠŸèƒ½

MATLABä»£ç è½¬Pythonå®ç°
åŒ…å«WMMSEä¼˜åŒ–ç®—æ³•å’Œä¿¡é“ç”ŸæˆåŠŸèƒ½
"""

# ä¸ºé¿å…ä¸å¤šè¿›ç¨‹å†²çªä»¥åŠé™ä½è¿‡åº¦çº¿ç¨‹åŒ–ï¼Œå¿…é¡»åœ¨å¯¼å…¥numpyå‰è®¾ç½®ç¯å¢ƒå˜é‡
import os as _os
_os.environ.setdefault('OMP_NUM_THREADS', '1')
_os.environ.setdefault('MKL_NUM_THREADS', '1')
_os.environ.setdefault('OPENBLAS_NUM_THREADS', '1')
_os.environ.setdefault('NUMEXPR_NUM_THREADS', '1')

import numpy as np
import scipy.io as sio
import os
import time
from typing import Tuple, Dict, Any, List
import warnings
import argparse
import multiprocessing
from concurrent.futures import ProcessPoolExecutor, as_completed
warnings.filterwarnings('ignore')

def generate_large_imac_dataset(workers: int = 1, max_iter: int = 150, tolerance: float = 1e-4, skip_estimate: bool = False,
                               batch_size: int = 10000, compress: bool = True, verbose_worker: bool = False, start_batch: int = 1):
    """ä¸»å‡½æ•°ï¼šç”Ÿæˆå¤§è§„æ¨¡IMACæ•°æ®é›†"""
    # å‚æ•°è®¾ç½®
    total_samples = 1000000
    train_samples = 990000
    test_samples = 10000
    
    # IMACå‚æ•°
    num_BS = 24
    num_User = 3
    R = 100
    minR_ratio = 0.2
    var_noise = 1
    
    # å­˜å‚¨è·¯å¾„
    save_path = '/root/autodl-tmp/IMAC/'
    if not os.path.exists(save_path):
        os.makedirs(save_path)
    
    print('å¼€å§‹ç”ŸæˆIMACæ•°æ®é›†...', flush=True)
    print(f'æ€»æ ·æœ¬æ•°: {total_samples} (è®­ç»ƒé›†: {train_samples}, æµ‹è¯•é›†: {test_samples})', flush=True)
    print(f'æ‰¹å¤§å°: {batch_size}', flush=True)
    print(f'å­˜å‚¨è·¯å¾„: {save_path}', flush=True)
    
    # é¢„ä¼°ç”Ÿæˆæ—¶é—´
    estimated_time = None
    if not skip_estimate:
        print('\n=== é¢„ä¼°ç”Ÿæˆæ—¶é—´ ===', flush=True)
        estimated_time = estimate_generation_time(
            num_BS, num_User, R, minR_ratio, var_noise, total_samples, batch_size,
            max_iter=max_iter, tolerance=tolerance
        )
    
    # ç”Ÿæˆè®­ç»ƒé›†
    print('\n=== ç”Ÿæˆè®­ç»ƒé›† ===', flush=True)
    generate_dataset_batches(
        total_samples=train_samples,
        batch_size=batch_size,
        num_BS=num_BS,
        num_User=num_User,
        R=R,
        minR_ratio=minR_ratio,
        var_noise=var_noise,
        save_path=save_path,
        dataset_type='train',
        estimated_time_per_sample=(estimated_time['time_per_sample'] if estimated_time else None),
        workers=workers,
        max_iter=max_iter,
        tolerance=tolerance,
        compress=compress,
        verbose_worker=verbose_worker,
        start_batch=start_batch,
    )
    
    # ç”Ÿæˆæµ‹è¯•é›†
    print('\n=== ç”Ÿæˆæµ‹è¯•é›† ===', flush=True)
    generate_dataset_batches(
        total_samples=test_samples,
        batch_size=batch_size,
        num_BS=num_BS,
        num_User=num_User,
        R=R,
        minR_ratio=minR_ratio,
        var_noise=var_noise,
        save_path=save_path,
        dataset_type='test',
        estimated_time_per_sample=(estimated_time['time_per_sample'] if estimated_time else None),
        workers=workers,
        max_iter=max_iter,
        tolerance=tolerance,
        compress=compress,
        verbose_worker=verbose_worker,
        start_batch=1,  # æµ‹è¯•é›†æ€»æ˜¯ä»ç¬¬1æ‰¹å¼€å§‹
    )
    
    print('\næ•°æ®é›†ç”Ÿæˆå®Œæˆï¼')

def estimate_generation_time(num_BS: int, num_User: int, R: int, minR_ratio: float, 
                           var_noise: float, total_samples: int, batch_size: int,
                           max_iter: int = 150, tolerance: float = 1e-4) -> Dict[str, float]:
    """é¢„ä¼°æ•°æ®ç”Ÿæˆæ—¶é—´"""
    print('æ­£åœ¨è¿è¡Œå°è§„æ¨¡æµ‹è¯•ä»¥é¢„ä¼°ç”Ÿæˆæ—¶é—´...')
    
    # æµ‹è¯•å‚æ•°ï¼ˆå¤šä¸ªè§„æ¨¡ï¼‰
    test_samples = [10, 50, 100]  # ä¸åŒçš„æµ‹è¯•æ ·æœ¬æ•°
    test_results = []
    
    for test_num in test_samples:
        print(f'  æµ‹è¯• {test_num} ä¸ªæ ·æœ¬...', end='')
        
        # è¿è¡Œå¤šæ¬¡å–å¹³å‡å€¼ä»¥æé«˜å‡†ç¡®æ€§
        times = []
        for trial in range(3):  # è¿è¡Œ3æ¬¡
            start_time = time.time()
            _, _, _, wmmse_time = Generate_IMAC_function_optimized(
                num_BS, num_User, test_num, R, minR_ratio, 
                42 + trial, var_noise, max_iter=max_iter, tolerance=tolerance  # ä¸åŒçš„éšæœºç§å­
            )
            total_time = time.time() - start_time
            times.append({
                'total': total_time,
                'wmmse': wmmse_time,
                'avg_per_sample': total_time / test_num,
                'avg_wmmse_per_sample': wmmse_time / test_num
            })
        
        # è®¡ç®—å¹³å‡å€¼
        avg_total = np.mean([t['total'] for t in times])
        avg_wmmse = np.mean([t['wmmse'] for t in times])
        avg_per_sample = avg_total / test_num
        avg_wmmse_per_sample = avg_wmmse / test_num
        
        test_results.append({
            'samples': test_num,
            'avg_per_sample': avg_per_sample,
            'avg_wmmse_per_sample': avg_wmmse_per_sample,
            'wmmse_ratio': avg_wmmse / avg_total
        })
        
        print(f' å¹³å‡æ¯æ ·æœ¬: {avg_per_sample:.4f}ç§’')
    
    # ä½¿ç”¨çº¿æ€§å›å½’é¢„æµ‹ï¼ˆè€ƒè™‘åˆ°å¯èƒ½çš„éçº¿æ€§å…³ç³»ï¼‰
    samples_array = np.array([r['samples'] for r in test_results])
    times_array = np.array([r['avg_per_sample'] for r in test_results])
    wmmse_times_array = np.array([r['avg_wmmse_per_sample'] for r in test_results])
    
    # è®¡ç®—åŠ æƒå¹³å‡ï¼ˆæ›´é‡è§†å¤§æ ·æœ¬çš„ç»“æœï¼‰
    weights = samples_array / np.sum(samples_array)
    estimated_time_per_sample = np.average(times_array, weights=weights)
    estimated_wmmse_per_sample = np.average(wmmse_times_array, weights=weights)
    
    # æ·»åŠ ç¼“å†²å› å­ï¼ˆè€ƒè™‘å¤§è§„æ¨¡è¿è¡Œæ—¶çš„é¢å¤–å¼€é”€ï¼‰
    buffer_factor = 1.1  # 10%çš„ç¼“å†²
    estimated_time_per_sample *= buffer_factor
    estimated_wmmse_per_sample *= buffer_factor
    
    # è®¡ç®—æ€»æ—¶é—´é¢„ä¼°
    total_estimated_time = estimated_time_per_sample * total_samples
    total_wmmse_time = estimated_wmmse_per_sample * total_samples
    
    # è®¡ç®—æ‰¹æ¬¡æ•°å’ŒI/Oæ—¶é—´
    num_batches = int(np.ceil(total_samples / batch_size))
    estimated_io_time = num_batches * 2  # å‡è®¾æ¯æ‰¹ä¿å­˜éœ€è¦2ç§’
    total_with_io = total_estimated_time + estimated_io_time
    
    # æ˜¾ç¤ºé¢„ä¼°ç»“æœ
    print('\næ—¶é—´é¢„ä¼°ç»“æœ:')
    print(f'  æ¯æ ·æœ¬å¹³å‡æ—¶é—´: {estimated_time_per_sample:.4f}ç§’')
    print(f'  å…¶ä¸­WMMSEæ—¶é—´: {estimated_wmmse_per_sample:.4f}ç§’ ({estimated_wmmse_per_sample/estimated_time_per_sample*100:.1f}%)')
    print(f'  ')
    print(f'  æ€»è®¡ç®—æ—¶é—´: {total_estimated_time/3600:.1f}å°æ—¶ ({total_estimated_time/60:.1f}åˆ†é’Ÿ)')
    print(f'  å…¶ä¸­WMMSEæ—¶é—´: {total_wmmse_time/3600:.1f}å°æ—¶')
    print(f'  é¢„ä¼°I/Oæ—¶é—´: {estimated_io_time/60:.1f}åˆ†é’Ÿ')
    print(f'  æ€»é¢„ä¼°æ—¶é—´: {total_with_io/3600:.1f}å°æ—¶ ({total_with_io/60:.1f}åˆ†é’Ÿ)')
    print(f'  ')
    print(f'  æ‰¹æ¬¡æ•°é‡: {num_batches}')
    print(f'  é¢„è®¡å®Œæˆæ—¶é—´: {time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(time.time() + total_with_io))}')
    
    # æ˜¾ç¤ºè¿›åº¦æé†’
    milestones = [0.1, 0.25, 0.5, 0.75, 0.9]
    print(f'  ')
    print('è¿›åº¦é‡Œç¨‹ç¢‘é¢„ä¼°:')
    for milestone in milestones:
        milestone_time = total_with_io * milestone
        completion_time = time.time() + milestone_time
        print(f'    {milestone*100:3.0f}%å®Œæˆ: {time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(completion_time))} '
              f'(çº¦{milestone_time/3600:.1f}å°æ—¶å)')
    
    # æ€§èƒ½å»ºè®®
    print(f'  ')
    print('æ€§èƒ½å»ºè®®:')
    if estimated_time_per_sample > 0.01:
        print('  âš ï¸  æ¯æ ·æœ¬æ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®åœ¨æ€§èƒ½è¾ƒå¥½çš„æœºå™¨ä¸Šè¿è¡Œ')
    if total_with_io > 12*3600:  # è¶…è¿‡12å°æ—¶
        print('  âš ï¸  é¢„è®¡ç”Ÿæˆæ—¶é—´è¶…è¿‡12å°æ—¶ï¼Œå»ºè®®åˆ†æ‰¹è¿è¡Œæˆ–é™ä½æ ·æœ¬æ•°')
    if estimated_wmmse_per_sample / estimated_time_per_sample > 0.8:
        print('  ğŸ’¡ WMMSEç®—æ³•å ç”¨å¤§éƒ¨åˆ†æ—¶é—´ï¼Œå¯è€ƒè™‘ç®—æ³•ä¼˜åŒ–')
    
    print('  âœ… å¯ä»¥è°ƒæ•´batch_sizeæ¥å¹³è¡¡å†…å­˜ä½¿ç”¨å’ŒI/Oæ•ˆç‡')
    
    # è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­
    try:
        user_input = input('\næ˜¯å¦ç»§ç»­ç”Ÿæˆå®Œæ•´æ•°æ®é›†ï¼Ÿ(y/n): ').lower().strip()
        if user_input not in ['y', 'yes', 'æ˜¯', '']:
            print('ç”¨æˆ·å–æ¶ˆç”Ÿæˆï¼Œç¨‹åºé€€å‡ºã€‚')
            exit(0)
    except KeyboardInterrupt:
        print('\nç”¨æˆ·ä¸­æ–­ï¼Œç¨‹åºé€€å‡ºã€‚')
        exit(0)
    
    return {
        'time_per_sample': estimated_time_per_sample,
        'wmmse_per_sample': estimated_wmmse_per_sample,
        'total_time': total_with_io,
        'compute_time': total_estimated_time,
        'io_time': estimated_io_time
    }

def generate_dataset_batches(total_samples: int, batch_size: int, num_BS: int, 
                           num_User: int, R: int, minR_ratio: float, 
                           var_noise: float, save_path: str, dataset_type: str,
                           estimated_time_per_sample: float = None,
                           workers: int = 1,
                           max_iter: int = 150,
                           tolerance: float = 1e-4,
                           compress: bool = True,
                           verbose_worker: bool = False,
                           start_batch: int = 1):
    """æ‰¹é‡ç”Ÿæˆæ•°æ®é›†ï¼ˆæ”¯æŒå¹¶è¡Œã€å¤šå‚æ•°æ§åˆ¶ï¼‰"""
    num_batches = int(np.ceil(total_samples / batch_size))
    total_wmmse_time = 0
    total_generation_time = 0
    
    # ç”¨äºè‡ªé€‚åº”æ—¶é—´é¢„æµ‹
    actual_times = []
    
    print(f'å¼€å§‹ç”Ÿæˆ {dataset_type}é›†: {total_samples} æ ·æœ¬ï¼Œåˆ† {num_batches} æ‰¹', flush=True)
    if start_batch > 1:
        print(f'æ–­ç‚¹ç»­ä¼ : ä»ç¬¬ {start_batch} æ‰¹å¼€å§‹ (è·³è¿‡å‰ {start_batch-1} æ‰¹)', flush=True)
        remaining_samples = total_samples - (start_batch - 1) * batch_size
        if remaining_samples > 0:
            print(f'å‰©ä½™æ ·æœ¬æ•°: {remaining_samples}', flush=True)
    if estimated_time_per_sample:
        estimated_total_time = estimated_time_per_sample * total_samples
        print(f'åŸºäºé¢„ä¼°ï¼Œé¢„è®¡æ€»æ—¶é—´: {estimated_total_time/60:.1f}åˆ†é’Ÿ', flush=True)
    print('-' * 60, flush=True)
    
    overall_start_time = time.time()
    
    if workers <= 1:
        for batch_idx in range(start_batch, num_batches + 1):
            if batch_idx == num_batches:
                current_batch_size = total_samples - (batch_idx - 1) * batch_size
            else:
                current_batch_size = batch_size

            print(f'[æ‰¹æ¬¡ {batch_idx:3d}/{num_batches}] æ­£åœ¨ç”Ÿæˆ {current_batch_size:5d} æ ·æœ¬...', end='', flush=True)
            stats = _generate_and_save_batch(
                batch_idx=batch_idx,
                current_batch_size=current_batch_size,
                num_BS=num_BS,
                num_User=num_User,
                R=R,
                minR_ratio=minR_ratio,
                var_noise=var_noise,
                save_path=save_path,
                dataset_type=dataset_type,
                max_iter=max_iter,
                tolerance=tolerance,
                compress=compress,
                verbose_worker=verbose_worker,
            )

            # ç´¯è®¡æ—¶é—´ç»Ÿè®¡
            total_wmmse_time += stats['wmmse_time']
            total_generation_time += stats['generation_time']

            # è®°å½•å®é™…æ—¶é—´ç”¨äºè‡ªé€‚åº”é¢„æµ‹
            actual_time_per_sample = stats['generation_time'] / current_batch_size
            actual_times.append(actual_time_per_sample)

            # å®Œæˆåº¦
            completed_samples = batch_idx * batch_size if batch_idx < num_batches else total_samples
            progress_percent = (completed_samples / total_samples) * 100
            print(f' å®Œæˆ! [{progress_percent:5.1f}%]', flush=True)

            # æ˜¾ç¤ºæ—¶é—´ä¿¡æ¯
            print(f'         ç”Ÿæˆæ—¶é—´: {stats["generation_time"]:6.2f}ç§’ | ä¿å­˜æ—¶é—´: {stats["save_time"]:5.2f}ç§’ | æ¯æ ·æœ¬: {actual_time_per_sample:.4f}ç§’', flush=True)

            # è‡ªé€‚åº”æ—¶é—´é¢„æµ‹
            if len(actual_times) >= 3:
                weights = np.array([0.2, 0.3, 0.5])
                recent_avg = np.average(actual_times[-3:], weights=weights)
            elif len(actual_times) >= 2:
                recent_avg = np.mean(actual_times[-2:])
            else:
                recent_avg = actual_times[0] if actual_times else estimated_time_per_sample

            if batch_idx < num_batches and recent_avg:
                remaining_samples = total_samples - completed_samples
                estimated_remaining_time = remaining_samples * recent_avg
                completion_time = time.time() + estimated_remaining_time
                completion_str = time.strftime("%H:%M:%S", time.localtime(completion_time))
                print(f'         å‰©ä½™æ—¶é—´: {estimated_remaining_time/60:6.1f}åˆ†é’Ÿ | é¢„è®¡å®Œæˆ: {completion_str}', flush=True)
                if estimated_time_per_sample and len(actual_times) >= 2:
                    performance_ratio = recent_avg / estimated_time_per_sample
                    if performance_ratio < 0.9:
                        print(f'         ğŸš€ æ€§èƒ½æ¯”é¢„æœŸå¥½ {(1-performance_ratio)*100:.0f}%', flush=True)
                    elif performance_ratio > 1.1:
                        print(f'         ğŸŒ æ€§èƒ½æ¯”é¢„æœŸæ…¢ {(performance_ratio-1)*100:.0f}%', flush=True)

            file_size_mb = os.path.getsize(stats['filename']) / (1024 * 1024)
            print(f'         æ–‡ä»¶å¤§å°: {file_size_mb:6.1f}MB | æ–‡ä»¶: {os.path.basename(stats["filename"])}', flush=True)
            print('', flush=True)
    else:
        # å¹¶è¡Œæ‰§è¡Œ
        print(f'å¹¶è¡Œå¯ç”¨: {workers} è¿›ç¨‹', flush=True)
        completed_samples = (start_batch - 1) * batch_size  # å·²å®Œæˆçš„æ ·æœ¬æ•°
        with ProcessPoolExecutor(max_workers=workers) as executor:
            futures = []
            for batch_idx in range(start_batch, num_batches + 1):
                if batch_idx == num_batches:
                    current_batch_size = total_samples - (batch_idx - 1) * batch_size
                else:
                    current_batch_size = batch_size
                print(f'æäº¤æ‰¹æ¬¡ {batch_idx:3d}/{num_batches} | æ ·æœ¬ {current_batch_size}', flush=True)
                futures.append(executor.submit(
                    _generate_and_save_batch,
                    batch_idx,
                    current_batch_size,
                    num_BS,
                    num_User,
                    R,
                    minR_ratio,
                    var_noise,
                    save_path,
                    dataset_type,
                    max_iter,
                    tolerance,
                    compress,
                    verbose_worker,
                ))

            for idx, future in enumerate(as_completed(futures), start=1):
                try:
                    stats = future.result()
                except Exception as e:
                    print(f'å­è¿›ç¨‹å¼‚å¸¸: {e}', flush=True)
                    raise
                completed_samples += stats['current_batch_size']
                total_wmmse_time += stats['wmmse_time']
                total_generation_time += stats['generation_time']

                progress_percent = (completed_samples / total_samples) * 100
                print(f'[æ‰¹æ¬¡ {stats["batch_idx"]:3d}/{num_batches}] å®Œæˆ! [{progress_percent:5.1f}%] | '
                      f'ç”Ÿæˆ: {stats["generation_time"]:6.2f}s | ä¿å­˜: {stats["save_time"]:5.2f}s | '
                      f'æ¯æ ·æœ¬: {stats["generation_time"]/stats["current_batch_size"]:.4f}s | '
                      f'æ–‡ä»¶: {os.path.basename(stats["filename"])}', flush=True)

                # è‡ªé€‚åº”æ—¶é—´é¢„æµ‹æ›´æ–°
                actual_times.append(stats['generation_time'] / stats['current_batch_size'])
                if len(actual_times) >= 3:
                    weights = np.array([0.2, 0.3, 0.5])
                    recent_avg = np.average(actual_times[-3:], weights=weights)
                elif len(actual_times) >= 2:
                    recent_avg = np.mean(actual_times[-2:])
                else:
                    recent_avg = actual_times[0] if actual_times else estimated_time_per_sample

                if completed_samples < total_samples and recent_avg:
                    remaining_samples = total_samples - completed_samples
                    estimated_remaining_time = remaining_samples * recent_avg
                    completion_time = time.time() + estimated_remaining_time
                    completion_str = time.strftime("%H:%M:%S", time.localtime(completion_time))
                    print(f'         å‰©ä½™æ—¶é—´: {estimated_remaining_time/60:6.1f}åˆ†é’Ÿ | é¢„è®¡å®Œæˆ: {completion_str}', flush=True)
                    if estimated_time_per_sample and len(actual_times) >= 2:
                        performance_ratio = recent_avg / estimated_time_per_sample
                        if performance_ratio < 0.9:
                            print(f'         ğŸš€ æ€§èƒ½æ¯”é¢„æœŸå¥½ {(1-performance_ratio)*100:.0f}%', flush=True)
                        elif performance_ratio > 1.1:
                            print(f'         ğŸŒ æ€§èƒ½æ¯”é¢„æœŸæ…¢ {(performance_ratio-1)*100:.0f}%', flush=True)
                print('', flush=True)
    
    # æ€»ä½“ç»Ÿè®¡
    overall_time = time.time() - overall_start_time
    avg_wmmse_time_total = total_wmmse_time / total_samples
    avg_generation_time_total = total_generation_time / total_samples
    avg_overall_time = overall_time / total_samples
    
    print('=' * 60)
    print(f'{dataset_type.upper()}é›†ç”Ÿæˆå®Œæˆ! ğŸ‰')
    print(f'æ€»æ ·æœ¬æ•°: {total_samples:,}')
    print(f'æ€»è€—æ—¶: {overall_time/60:.2f}åˆ†é’Ÿ ({overall_time/3600:.2f}å°æ—¶)')
    print(f'çº¯è®¡ç®—æ—¶é—´: {total_generation_time/60:.2f}åˆ†é’Ÿ')
    print(f'å¹³å‡æ¯æ ·æœ¬: {avg_overall_time:.4f}ç§’ (å«I/O)')
    print(f'å¹³å‡æ¯æ ·æœ¬: {avg_generation_time_total:.4f}ç§’ (çº¯è®¡ç®—)')
    print(f'å¹³å‡WMMSEæ—¶é—´: {avg_wmmse_time_total:.4f}ç§’')
    print(f'WMMSEå æ¯”: {(total_wmmse_time / total_generation_time) * 100:.1f}%')
    
    # æ€§èƒ½è¯„ä¼°
    if estimated_time_per_sample:
        actual_vs_estimated = avg_generation_time_total / estimated_time_per_sample
        print(f'å®é™…vsé¢„ä¼°: {actual_vs_estimated:.2f}x '
              f'({"å¿«äº" if actual_vs_estimated < 1 else "æ…¢äº"}é¢„æœŸ)')
    
    print('=' * 60)

def Generate_IMAC_function_optimized(num_BS: int, num_User: int, num_H: int, 
                                   R: int, minR_ratio: float, seed: int, 
                                   var_noise: float, max_iter: int = 150, tolerance: float = 1e-4,
                                   verbose: bool = False) -> Tuple[np.ndarray, np.ndarray, np.ndarray, float]:
    """ä¼˜åŒ–çš„IMACå‡½æ•°ç”Ÿæˆ"""
    np.random.seed(seed)
    K = num_User * num_BS
    
    # é¢„åˆ†é…å†…å­˜
    X = np.zeros((K * num_BS, num_H), dtype=np.float32)
    Y = np.zeros((K, num_H), dtype=np.float32)
    H = np.zeros((K * K, num_H), dtype=np.float32)
    
    # é¢„è®¡ç®—Cellç»“æ„ï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
    Cell = setup_cell_structure(num_BS, R)
    
    total_wmmse_time = 0
    
    # æ‰¹é‡ç”Ÿæˆä¿¡é“å’Œè®¡ç®—WMMSE
    progress_interval = max(1, num_H // 20)  # 5% ä¸€æ¬¡
    for i in range(num_H):
        # ç”Ÿæˆä¿¡é“çŸ©é˜µ (å•æ ·æœ¬)
        H_eq = generate_IBC_channel_optimized(num_User, R, num_BS, minR_ratio, Cell)
        H[:, i] = H_eq.reshape(-1).astype(np.float32)
        
        # é‡ç»„ä¿¡é“çŸ©é˜µç”¨äºè¾“å…¥
        temp_H = reshape_channel_matrix(H_eq, num_BS, num_User)
        X[:, i] = temp_H.flatten()
        
        # è®¡ç®—WMMSEï¼ˆè®°å½•æ—¶é—´ï¼‰
        wmmse_start = time.time()
        Y[:, i] = WMMSE_sum_rate_optimized(
            p_int=np.ones(K, dtype=np.float32),
            H=H_eq,
            Pmax=np.ones(K, dtype=np.float32),
            var_noise=np.float32(var_noise),
            max_iter=max_iter,
            tolerance=tolerance,
        )
        wmmse_time = time.time() - wmmse_start
        total_wmmse_time += wmmse_time
        if verbose and ((i + 1) % progress_interval == 0 or (i + 1) == num_H):
            print(f'  å­ä»»åŠ¡è¿›åº¦: {(i+1):6d}/{num_H} æ ·æœ¬', flush=True)
    
    return H, X, Y, total_wmmse_time

def setup_cell_structure(Num_of_cell: int, cell_distance: int) -> Dict[str, Any]:
    """é¢„è®¡ç®—Cellç»“æ„ï¼Œé¿å…åœ¨æ¯æ¬¡è°ƒç”¨æ—¶é‡å¤è®¡ç®—"""
    Cell = {
        'Ncell': Num_of_cell,
        'Nintra': 3,  # å›ºå®šå€¼
        'NintraBase': 1,  # å›ºå®šå€¼
        'Rcell': cell_distance * 2 / np.sqrt(3)
    }
    
    # é¢„è®¡ç®—Cellä½ç½®
    Cell['Position'] = compute_cell_positions(Cell['Ncell'], Cell['Rcell'])
    
    return Cell

def compute_cell_positions(Nbs: int, Rcell: float) -> np.ndarray:
    """ä¼˜åŒ–çš„Cellä½ç½®è®¡ç®—"""
    positions = np.zeros((Nbs, 2))
    
    if Nbs > 1:
        theta = np.arange(Nbs - 1) * np.pi / 3
        positions[1:, :] = np.sqrt(3) * Rcell * np.column_stack([np.cos(theta), np.sin(theta)])
    
    if Nbs > 7:
        theta1 = np.arange(-np.pi/6, 5*np.pi/3, np.pi/3)
        theta2 = np.arange(0, 5*np.pi/3, np.pi/3)
        x = np.concatenate([3*Rcell*np.cos(theta1), 2*np.sqrt(3)*Rcell*np.cos(theta2)])
        y = np.concatenate([3*Rcell*np.sin(theta1), 2*np.sqrt(3)*Rcell*np.sin(theta2)])
        
        end_idx = min(19, Nbs)
        positions[7:end_idx, :] = np.column_stack([x[:end_idx-7], y[:end_idx-7]])
    
    return positions

def reshape_channel_matrix(H_matrix: np.ndarray, num_BS: int, num_User: int) -> np.ndarray:
    """ä¼˜åŒ–çš„ä¿¡é“çŸ©é˜µé‡ç»„"""
    K = num_User * num_BS
    temp_H = np.zeros((num_BS, K), dtype=np.float32)
    
    for l in range(num_BS):
        temp_H[l, :] = H_matrix[l * num_User, :]
    
    return temp_H.reshape(-1, 1)

def WMMSE_sum_rate_optimized(p_int: np.ndarray, H: np.ndarray, 
                           Pmax: np.ndarray, var_noise: float,
                           max_iter: int = 150, tolerance: float = 1e-4) -> np.ndarray:
    """ä¼˜åŒ–çš„WMMSEç®—æ³•ï¼ˆå‘é‡åŒ–ä¸float32åŠ é€Ÿç‰ˆï¼‰"""
    K = Pmax.shape[0]
    # ä½¿ç”¨ float32 åŠ é€Ÿå¹¶å‡å°‘å†…å­˜
    b = np.sqrt(p_int.astype(np.float32))
    H = H.astype(np.float32)
    Pmax = Pmax.astype(np.float32)
    var_noise = np.float32(var_noise)

    # é¢„è®¡ç®—
    H_diag = np.diag(H).astype(np.float32)
    H_squared = (H * H).astype(np.float32)

    f = np.zeros(K, dtype=np.float32)
    w = np.zeros(K, dtype=np.float32)

    # åˆå§‹åŒ–ä¸€æ¬¡ f,w,v
    b_squared = b * b
    denom_fw = H_squared @ b_squared + var_noise
    f = (H_diag * b) / denom_fw
    w = 1.0 / (1.0 - f * b * H_diag)
    vnew = np.sum(np.log2(w))

    for _ in range(max_iter):
        vold = vnew
        # æ›´æ–° bï¼ˆå®Œå…¨å‘é‡åŒ–ï¼‰
        weights = w * (f * f)  # å½¢çŠ¶ (K,)
        denom_b = H_squared.T @ weights  # å½¢çŠ¶ (K,)
        # é¿å…é™¤é›¶
        denom_b = np.where(denom_b <= 1e-12, 1e-12, denom_b).astype(np.float32)
        numerator = w * f * H_diag
        btmp = numerator / denom_b
        b = np.clip(btmp, 0.0, np.sqrt(Pmax))

        # æ›´æ–° f, w, vï¼ˆå‘é‡åŒ–ï¼‰
        b_squared = b * b
        denom_fw = H_squared @ b_squared + var_noise
        f = (H_diag * b) / denom_fw
        w = 1.0 / (1.0 - f * b * H_diag)
        vnew = np.sum(np.log2(w))

        if np.abs(vnew - vold) <= tolerance:
            break

    p_opt = b * b
    return p_opt.astype(np.float32)

def _generate_and_save_batch(batch_idx: int,
                            current_batch_size: int,
                            num_BS: int,
                            num_User: int,
                            R: int,
                            minR_ratio: float,
                            var_noise: float,
                            save_path: str,
                            dataset_type: str,
                            max_iter: int,
                            tolerance: float,
                            compress: bool,
                            verbose_worker: bool) -> Dict[str, Any]:
    """å­è¿›ç¨‹/ä¸»è¿›ç¨‹é€šç”¨ï¼šç”Ÿæˆä¸€ä¸ªæ‰¹æ¬¡å¹¶ä¿å­˜åˆ°ç£ç›˜ï¼Œè¿”å›ç»Ÿè®¡ä¿¡æ¯"""
    batch_start_time = time.time()
    # ç”Ÿæˆå½“å‰æ‰¹æ¬¡æ•°æ®
    H_batch, X_batch, Y_batch, wmmse_time = Generate_IMAC_function_optimized(
        num_BS, num_User, current_batch_size, R, minR_ratio, batch_idx, var_noise,
        max_iter=max_iter, tolerance=tolerance, verbose=verbose_worker)

    # ä¿å­˜æ•°æ®
    filename = f'{save_path}{dataset_type}_batch_{batch_idx:03d}.mat'
    save_start_time = time.time()
    sio.savemat(
        filename,
        {
            'H_batch': H_batch.astype(np.float16, copy=False),
            'X_batch': X_batch.astype(np.float16, copy=False),
            'Y_batch': Y_batch.astype(np.float16, copy=False),
        },
        do_compression=bool(compress),
        long_field_names=False,
    )
    save_time = time.time() - save_start_time
    total_time = time.time() - batch_start_time
    generation_time = total_time - save_time
    return {
        'batch_idx': batch_idx,
        'current_batch_size': current_batch_size,
        'wmmse_time': wmmse_time,
        'generation_time': generation_time,
        'save_time': save_time,
        'total_time': total_time,
        'filename': filename,
    }

def initialize_wmmse_variables(b: np.ndarray, H: np.ndarray, H_diag: np.ndarray, 
                             H_squared: np.ndarray, var_noise: float, 
                             f: np.ndarray, w: np.ndarray) -> float:
    """åˆå§‹åŒ–WMMSEå˜é‡"""
    K = len(b)
    b_squared = b * b
    denom = H_squared @ b_squared + var_noise
    f[:K] = (H_diag * b) / denom
    w[:K] = 1.0 / (1.0 - f * b * H_diag)
    return float(np.sum(np.log2(w)))

def update_fwv_variables(b: np.ndarray, H: np.ndarray, H_diag: np.ndarray, 
                        H_squared: np.ndarray, var_noise: float, 
                        f: np.ndarray, w: np.ndarray) -> float:
    """æ›´æ–°WMMSEå˜é‡"""
    b_squared = b * b
    denom = H_squared @ b_squared + var_noise
    f[:] = (H_diag * b) / denom
    w[:] = 1.0 / (1.0 - f * b * H_diag)
    return float(np.sum(np.log2(w)))

def generate_IBC_channel_optimized(Num_of_user_in_each_cell: int, cell_distance: int, 
                                 Num_of_cell: int, minR_ratio: float, 
                                 Cell: Dict[str, Any]) -> np.ndarray:
    """ä¼˜åŒ–çš„ä¿¡é“ç”Ÿæˆå‡½æ•°"""
    T = 1
    BaseNum = 1
    UserNum = Num_of_user_in_each_cell
    CellNum = Num_of_cell
    
    # ç”Ÿæˆç”¨æˆ·å’ŒåŸºç«™ä½ç½®
    MS, BS = usergenerator_optimized(Cell, minR_ratio)
    
    # è®¡ç®—å¤§å°ºåº¦è¡°è½
    HLarge = channelsample_optimized(BS, MS, Cell)
    
    # ç”Ÿæˆå°å°ºåº¦è¡°è½
    H_small_real = np.random.randn(T, BaseNum, CellNum, UserNum, CellNum).astype(np.float32)
    H_small_imag = np.random.randn(T, BaseNum, CellNum, UserNum, CellNum).astype(np.float32)
    _scale = np.float32(1.0 / np.sqrt(np.float32(2.0)))
    H_small = (H_small_real + 1j * H_small_imag) * _scale
    
    # åˆå¹¶å¤§å°å°ºåº¦è¡°è½
    total_user_Num = CellNum * UserNum
    H_eq = np.zeros((total_user_Num, total_user_Num), dtype=np.float32)
    
    k = 0
    for CellMS in range(CellNum):
        for User in range(UserNum):
            k_INF = 0
            for INFCellMS in range(CellNum):
                for INFUser in range(UserNum):
                    H_combined = (H_small[0, BaseNum-1, CellMS, INFUser, INFCellMS] * 
                                np.sqrt(HLarge[INFUser, INFCellMS, BaseNum-1, CellMS]).astype(np.float32))
                    H_eq[k, k_INF] = np.abs(H_combined).astype(np.float32)
                    k_INF += 1
            k += 1
    
    return H_eq

def usergenerator_optimized(Cell: Dict[str, Any], minR_ratio: float) -> Tuple[Dict, Dict]:
    """ä¼˜åŒ–çš„ç”¨æˆ·ç”Ÿæˆå™¨"""
    Ncell = Cell['Ncell']
    Nintra = Cell['Nintra']
    NintraBase = Cell['NintraBase']
    Rcellmin = minR_ratio * Cell['Rcell']
    
    MS = {'Position': [None] * Ncell}
    BS = {'Position': [None] * Ncell}
    
    # æ‰¹é‡ç”Ÿæˆéšæœºæ•°
    all_theta = np.random.rand(Nintra * Ncell) * 2 * np.pi
    theta_idx = 0
    
    # ç”Ÿæˆç”¨æˆ·ä½ç½®
    for n in range(Ncell):
        theta = all_theta[theta_idx:theta_idx + Nintra]
        theta_idx += Nintra
        
        x, y = distrnd_optimized(Cell['Rcell'], Rcellmin, theta)
        MS['Position'][n] = np.column_stack([x + Cell['Position'][n, 0], 
                                           y + Cell['Position'][n, 1]])
    
    # ç”ŸæˆåŸºç«™ä½ç½®
    for n in range(Ncell):
        BS['Position'][n] = Cell['Position'][n, :].reshape(1, -1)
    
    return MS, BS

def distrnd_optimized(Rcell: float, Rcellmin: float, theta: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """ä¼˜åŒ–çš„è·ç¦»ç”Ÿæˆå‡½æ•°"""
    MsNum = len(theta)
    R = Rcell - Rcellmin
    
    # æ‰¹é‡ç”Ÿæˆè·ç¦»
    d = np.sum(np.random.rand(MsNum, 2), axis=1) * R
    d[d > R] = 2 * R - d[d > R]
    d = d + Rcellmin
    
    # è®¡ç®—åæ ‡
    x = d * np.cos(theta)
    y = d * np.sin(theta)
    
    return x, y

def channelsample_optimized(BS: Dict, MS: Dict, Cell: Dict[str, Any]) -> np.ndarray:
    """ä¼˜åŒ–çš„ä¿¡é“é‡‡æ ·"""
    Ncell = Cell['Ncell']
    Nintra = Cell['Nintra']
    Nbs = Cell['NintraBase']
    
    Hlarge = np.zeros((Nintra, Ncell, Nbs, Ncell), dtype=np.float32)
    
    # é¢„ç”Ÿæˆæ‰€æœ‰éšæœºæ•°
    all_randn = (np.random.randn(Nintra * Ncell * Nbs * Ncell).astype(np.float32)) * (8.0/10.0)
    randn_idx = 0
    
    for CellBS in range(Ncell):
        for CellMS in range(Ncell):
            for Base in range(Nbs):
                for User in range(Nintra):
                    d = np.linalg.norm(MS['Position'][CellMS][User, :] - 
                                     BS['Position'][CellBS][Base, :])
                    PL = 10**(all_randn[randn_idx]) * (200/d)**3
                    randn_idx += 1
                    Hlarge[User, CellMS, Base, CellBS] = PL
    
    return Hlarge

def quick_test_generation(workers: int = 1, max_iter: int = 150, tolerance: float = 1e-4):
    """å¿«é€Ÿæµ‹è¯•ç”Ÿæˆï¼ˆè·³è¿‡æ—¶é—´é¢„ä¼°ï¼‰"""
    # å°è§„æ¨¡æµ‹è¯•å‚æ•°
    total_samples = 100
    train_samples = 80
    test_samples = 20
    batch_size = 50
    
    # IMACå‚æ•°
    num_BS = 6
    num_User = 3
    R = 100
    minR_ratio = 0.2
    var_noise = 1
    
    # å­˜å‚¨è·¯å¾„
    save_path = '/root/autodl-tmp/test_output/'
    if not os.path.exists(save_path):
        os.makedirs(save_path)
    
    print('å¿«é€Ÿæµ‹è¯•æ¨¡å¼ - ç”Ÿæˆå°è§„æ¨¡IMACæ•°æ®é›†...')
    print(f'æ€»æ ·æœ¬æ•°: {total_samples} (è®­ç»ƒé›†: {train_samples}, æµ‹è¯•é›†: {test_samples})')
    print(f'å­˜å‚¨è·¯å¾„: {save_path}')
    
    start_time = time.time()
    
    # ç”Ÿæˆè®­ç»ƒé›†
    print('\n=== ç”Ÿæˆè®­ç»ƒé›† ===')
    generate_dataset_batches(train_samples, batch_size, num_BS, num_User, R, 
                           minR_ratio, var_noise, save_path, 'train',
                           estimated_time_per_sample=None,
                           workers=workers,
                           max_iter=max_iter,
                           tolerance=tolerance)
    
    # ç”Ÿæˆæµ‹è¯•é›†
    print('\n=== ç”Ÿæˆæµ‹è¯•é›† ===')
    generate_dataset_batches(test_samples, batch_size, num_BS, num_User, R, 
                           minR_ratio, var_noise, save_path, 'test',
                           estimated_time_per_sample=None,
                           workers=workers,
                           max_iter=max_iter,
                           tolerance=tolerance)
    
    total_time = time.time() - start_time
    print(f'\nå¿«é€Ÿæµ‹è¯•å®Œæˆ! æ€»è€—æ—¶: {total_time:.2f}ç§’')

if __name__ == '__main__':
    try:
        multiprocessing.set_start_method('spawn', force=True)
    except RuntimeError:
        pass
    parser = argparse.ArgumentParser(description='IMAC æ•°æ®é›†ç”Ÿæˆå™¨')
    parser.add_argument('--quick', action='store_true', help='å¿«é€Ÿå°è§„æ¨¡æµ‹è¯•ç”Ÿæˆ')
    parser.add_argument('--workers', type=int, default=max(1, multiprocessing.cpu_count() - 1), help='å¹¶è¡Œè¿›ç¨‹æ•°')
    parser.add_argument('--batch-size', type=int, default=10000, help='æ¯æ‰¹æ ·æœ¬æ•°')
    parser.add_argument('--max-iter', type=int, default=150, help='WMMSE æœ€å¤§è¿­ä»£æ¬¡æ•°')
    parser.add_argument('--tolerance', type=float, default=1e-4, help='WMMSE æ”¶æ•›é˜ˆå€¼')
    parser.add_argument('--no-compress', action='store_true', help='ä¿å­˜æ—¶ä¸å‹ç¼©ï¼ˆæ›´å¿«ï¼Œæ–‡ä»¶æ›´å¤§ï¼‰')
    parser.add_argument('--verbose-worker', action='store_true', help='å­è¿›ç¨‹è¾“å‡ºå†…éƒ¨è¿›åº¦ï¼ˆæ’éšœç”¨ï¼‰')
    parser.add_argument('--estimate', action='store_true', help='è¿è¡Œå‰åšæ—¶é—´é¢„ä¼°ï¼ˆå¯èƒ½è¾ƒæ…¢ä¸”éœ€è¦äº¤äº’ç¡®è®¤ï¼‰')
    parser.add_argument('--start-batch', type=int, default=1, help='ä»ç¬¬å‡ æ‰¹å¼€å§‹ç”Ÿæˆï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰')
    args = parser.parse_args()

    if args.quick:
        quick_test_generation(workers=args.workers, max_iter=args.max_iter, tolerance=args.tolerance)
    else:
        generate_large_imac_dataset(
            workers=args.workers,
            max_iter=args.max_iter,
            tolerance=args.tolerance,
            skip_estimate=(not args.estimate),
            batch_size=args.batch_size,
            compress=(not args.no_compress),
            verbose_worker=args.verbose_worker,
            start_batch=args.start_batch,
        )

